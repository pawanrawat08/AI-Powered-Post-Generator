[
    {
        "text": "Did you know Stripe has 4 tiers of Rate Limiting? \ud83e\udd2f\n\nStripe, being a payments platform, needs to be highly available no matter what. More importantly, a load from one of their customers or a surge in traffic should not crash the system.\n\nRecently, I dissected their engineering blog where they explained their rate-limiting architecture. Surprisingly, they have 4 tiers of rate limiters, and it is a pretty neat concept.\n\nToday, I published the video dissecting and explaining the tiers and how they help to keep infrastructure highly available.\n\ngive it a watch - youtu.be/Oxy-6MAiYPw\n\nHope this helps you build better systems at your workplace. Like always, absorb the patterns",
        "engagement": 697,
        "line_count": 9,
        "language": "English",
        "tags": [
            "Stripe",
            "Rate Limiting"
        ]
    },
    {
        "text": "How far can you go vibecoding? I just built LevelCache, an ephemeral cache built on top of LevelDB with TTL support - all vibecoded :)\n\nIt's a C library, so it can be easily integrated into any language through language bindings. This cache is designed to optimize for a different set of trade-offs - for example, it holds data on disk rather than in memory, allowing you to cache much more data and save on expensive network calls.\n\nThe repo also contains tests, benchmarks, examples, and documentation, and will be production-ready (though not yet adopted) in the coming days as I integrate it into a couple of prod codebases.\n\nAll of this was done using VSCode with the Gemini CLI.\n\nThe key is to break down what needs to be done into small, \"mundane-enough\" tasks. So, what you really need is clarity on what needs to be done and how.\n\nrepo - https://lnkd.in/gTjKXBVN\n\nIn case you want to take a look and give it a try, there's an examples directory in the repo to help you get started.\n\nHope this helps.",
        "engagement": 700,
        "line_count": 11,
        "language": "English",
        "tags": [
            "Vibe Coding",
            "System Design"
        ]
    },
    {
        "text": "C code is so much more readable than C++.\n\nOne skim of a C codebase and you can sense the whole flow, but somehow a C++ codebase always seems a bit more convoluted.",
        "engagement": 400,
        "line_count": 2,
        "language": "English",
        "tags": [
            "C++",
            "C"
        ]
    },
    {
        "text": "Time-series databases use a really simple algorithm to compress the data and reduce storage footprint by ~10x.\n\nDelta Compression (encoding) is one of the simplest algorithms for compressing a series of numerical data. Most time-series databases use this (or some variant like delta-delta) to reduce their storage footprints without losing much query capabilities.\n\nThe core idea is to store the difference between the current and previous points instead of the actual data point. Given a low variance in most time-series data (ex: CPU utilization, API response times, etc), the numbers can be represented in far fewer bits than a standard 32/64 bit representation.\n\nI would highly recommend you spend some time coding this to understand it better and benchmark the reduction in storage. On a random set of monotonically increasing integers, I saw an 8x reduction in storage space, which is pretty impressive for something that can be implemented in a mere 15 lines of code.\n\nYou will always understand more by coding than by reading. So, build quick prototypes of any and every system or concept you come across.\n\nHope it helps.",
        "engagement": 366,
        "line_count": 11,
        "language": "English",
        "tags": [
            "Database",
            "Data Compression"
        ]
    },
    {
        "text": "My usual day at work -\n\npick some task\nwrite a big-ass prompt\nlet Gemini do the heavy lifting\nwhile I sip my coffee\nmake some edits\npush and publish\n\nmy life has been reduced to prompting; and doing it well. no complaints \ud83d\ude05 bigger impact, less effort.",
        "engagement": 1300,
        "line_count": 9,
        "language": "English",
        "tags": [
            "AI",
            "Productivity"
        ]
    },
    {
        "text": "The worst way to teach/interview/learn system design is to design the entire company (ex: Quora, Airbnb, PayPal, Paytm, Hotstar) \ud83e\udd26\u200d\u2642\ufe0f\n\nWhen you pick such a big system, you will create hypothetical Microservices and will not have time to go into engineering depth.\n\nA good system design discussion is when a problem statement is focused. You get a chance to discuss the component in-depth and touch upon the engineering aspects like bottlenecks, locking, and transactions.\n\nYahi to asli maza hai! Tackling engineering challenges at scale.",
        "engagement": 1280,
        "line_count": 6,
        "language": "Hinglish",
        "tags": [
            "Software Engineer Career",
            "System Design"
        ]
    },
    {
        "text": "Sometime back, I was going through PostgreSQL internals and found something interesting about the `create database` command ...\n\nWhen we create a new database in PostgreSQL with our standard `create database` command, internally it copies an existing database named `template1` and creates a new one. These are called Template Databases.\n\nGiven that the new database is a copy of a template database, if we add some tables, rows, or stored procedures to this `template1` database, they will also be copied when we create a new database. This is a good way to bootstrap subsequent databases with any tables, data, stored procedures, or functions we need.\n\nThe good thing is that we can also create as many additional template databases as we like and specify them as a template when we create a new one, like this\n\n```\ncreate database new_db template mono_tmpl_1;\n```\n\nThe core advantages of having a custom template database are\n\n1. bootstraps a new database with tables, functions, and settings\n2. ensures consistency across all the databases in a cluster\n3. minimizes the risk of variations and errors in setting up new databases\n\nIt is always fun going through database internals, you always learn something new and amusing :)",
        "engagement": 809,
        "line_count": 19,
        "language": "English",
        "tags": [
            "Database"
        ]
    },
    {
        "text": "Why Interviews Are Biased And What Sets You Apart.\n\nInterviews are unfair and will remain so, no matter how hard we try they can never evaluate people 100% objectively. But there are a few traits that consistently help you stand out.\n\nI covered this in detail, so give it a read.\n\nAlso, I shared a video titled \"Sharing Databases in Microservices: Anti-Pattern or Practical?\" In this video, I talk about when sharing a database actually makes sense, how to mitigate the trade-offs, and why it\u2019s not always an anti-pattern.\n\nI also shared a paper I read titled RadixZip: Linear Time Compression of Token Streams. It\u2019s a fast, structure-aware compression algorithm that performs better than bzip2 on structured data like logs. Pretty neat trade-offs.\n\nAs usual, I also shared the three articles that I read and would highly recommend:\n\n- You're all CTO now\n- Caching is an Abstraction, not an Optimization\n- How often is the query plan optimal?",
        "engagement": 100,
        "line_count": 9,
        "language": "English",
        "tags": [
            "Software Engineer Career",
            "Microservices"
        ]
    },
    {
        "text": "Why do most Bloom Filters use MurmurHash over SHA or MD5?\n\nAll bloom filter operations (get, put) require the key to be passed through the hash function(s) to determine a position in the filter.\n\nFor each operation, the hash function will be invoked multiple times, typically thrice, and hence performance is essential. On the other hand, more popular hash functions like SHA and MD5 are cryptographic, which means they are designed to be\n\n- highly secure\n- well-distributed with a low collision rate\n\nEnsuring these guarantees makes computing SHA and MD5 expensive.\n\nMurmurHash, on the other side, is a non-cryptographic hash function that is optimized for speed and hence is often used in scenarios where speed is critical, such as bloom filters, hash tables, and caches.\n\nBecause Bloom Filters has no requirement of being secure, we can trade security for performance and hence MurmurHash becomes the default choice.",
        "engagement": 450,
        "line_count": 9,
        "language": "English",
        "tags": [
            "Database",
            "Hashing Algorithm"
        ]
    },
    {
        "text": "A few weeks back, I was reading about NUMA. After some quick LLM lookups, I built a primitive understanding and wanted to see where and how it fits.\n\nThis week, I am reading a paper from Google that digs deep into how microarchitectural decisions, like memory locality, impact performance at scale. The paper is titled \"Optimizing Google\u2019s Warehouse-Scale Computers: The NUMA Experience.\n\nThe core question it asks: when you're operating at Google scale, is ignoring NUMA locality leaving serious performance on the table? Turns out, the answer is yes.\n\nThe paper talks about how they profiled live data center workloads under load to understand how NUMA interacts with real applications and real constraints.\n\nIf you have worked with scheduling, load balancing, or performance tuning in complex systems, this paper will make you rethink your assumptions. It's not just about NUMA - it is about how low-level interactions affect performance in ways we often miss. Worth a weekend read.",
        "engagement": 150,
        "line_count": 9,
        "language": "English",
        "tags": [
            "NUMA",
            "Performance"
        ]
    },
    {
        "text": "AI has certainly made me 2x engg of what I was a year ago.\n\nIt has improved the quality of my thinking and articulation. Everything I once wanted to explore, I'm now diving into with ease and, more importantly, asking non-trivial deeper questions along the way.\n\nThe impact of AI is likely to follow a bimodal distribution, as those at the extremes will benefit from it the most.",
        "engagement": 1002,
        "line_count": 3,
        "language": "English",
        "tags": [
            "AI",
            "Technology"
        ]
    },
    {
        "text": "Should you learn sharding and partitioning if we already have a bunch of managed databases? According to me, an absolute yes! But more than that, you should know how to implement them. A few resources I highly recommend\n\n1. Even when the database is managed, you can get a better performance out of existing managed MySQL and Postgres by partitioning the data, helping you improve UX and reduce cost.\n\nRead this explanation on StackOverflow: https://lnkd.in/gPVC8emn\n\n2. Knowing how the data is laid out on the disk and nodes to which each partition is assigned, you can pick the right partitioning key, ensuring you get the best performance even from the managed database without requiring overprovisioning it.\n\nRead: https://lnkd.in/gbJBVXFF\n\n3. The concept of data partitioning is very similar to split ownership, which you would require on the compute side to understand how you would do distributed computing (and even single-node multi-threaded programs) such that you maximize the hardware utilization.\n\nWatch: youtu.be/2PjlaUnrAMQ\n\n4. Knowing these is also essential when you work with Spark because even though there are managed Spark offerings, by partitioning your data well and understanding the Physical Plan, you can bring down your Spark job execution time and, in turn, the cost.\n\nRead about: Broadcast Nested Loop Join\n\n5. Knowing how to partition data without downtime and move shards across without downtime is something that will help you do no-downtime database migrations (from one DB to another).\n\nWatch: youtu.be/9iAJjtvBwyI\n\nI believe these are the concepts that you should know in and out because now distributed computing and split ownership patterns are becoming quite common, and Sharding and Partitioning form the crux of it, but you and others may have a different opinion on this.\n\nIn case you want to understand Sharding and Partitioning from absolute scratch and the real difference between watch youtu.be/wXvljefXyEo.",
        "engagement": 420,
        "line_count": 34,
        "language": "English",
        "tags": [
            "Database",
            "System Design"
        ]
    },
    {
        "text": "High fork times can lead to master unavailability in Redis/Valkey, which is due to the large page table that needs to be copied during a fork.\n\nEven though the fork operation doesn't copy the actual pages and uses copy-on-write, the page table still needs to be copied.\n\nIf the process's memory consumption is high, the page table is relatively bigger, and copying it can take several seconds, blocking the main thread.\n\nSo, all we need to do is optimize the fork time while making sure there are no other repercussions.\n\nWorking at scale hits hard... all those OS concepts suddenly come in handy when you're optimizing this stuff :)",
        "engagement": 220,
        "line_count": 6,
        "language": "English",
        "tags": [
            "Database"
        ]
    },
    {
        "text": "Address matching is a super interesting problem and something I highly recommend trying to solve from scratch. The core idea is to determine whether two addresses refer to the same location, for example\n\n - 221B Baker St., London NW1 6XE, UK\n - 221-B Baker Street, NW1 6XE London, United Kingdom\n\nThe core problem statement is to identify if two slightly different-looking address strings actually refer to the same physical location. Things become interesting as you would need to accommodate for typos, abbreviations, formatting differences, missing components, or even language variations.\n\nIf you give this a shot, you will learn about \n\n - fuzzy string matching\n - normalizing and parsing text data\n - rule-based vs machine learning-based entity resolution\n - improve match accuracy with geospatial data\n\nAgain, use the LLM of your choice, but make sure you dig deeper and understand all the nuances.\n\nHope it helps. Have fun.",
        "engagement": 1032,
        "line_count": 11,
        "language": "English",
        "tags": [
            "String Matching Algorithm"
        ]
    },
    {
        "text": "CLI >>>>> GUI\n\nThere is some rare joy in using a good CLI app; that no GUI, no matter how flashy, can ever come close to.",
        "engagement": 1087,
        "line_count": 2,
        "language": "English",
        "tags": [
            "CLI",
            "GUI"
        ]
    },
    {
        "text": "Sure, horizontal scaling is great for handling large loads and provides fault tolerance, but it comes with its own set of challenges. Let's talk about them today...\n\nOne of the biggest challenges with horizontally scaled systems is maintaining data consistency across multiple machines. This complexity of coordination slows down the system, ultimately affecting throughput.\n\nAnother issue is dealing with network-related problems. Disconnected machines, network congestion, and outages are not uncommon. So when you are making your system horizontally scalable, it is critical to understand\n\n - how will data be brought back in sync\n - how will data integrity be ensured\n - how quickly can you recover from an outage\n - what happens if the data never fully syncs\n\nThen, of course, there is the question of data and workload ownership. With multiple machines handling the load, how do you decide which one gets what? What if one machine becomes overloaded - a \"hot node\"? How will you manage that?\n\nWith a larger infrastructure footprint, you would need to consider observability and monitoring. Without it, you'll have no idea what went wrong or where. Yes, distributed systems can be painful.\n\nRemember, vertical scaling is still a valid approach, as long as it solves your problem. At the end of the day, your job is to solve problems, not just write code or build complex infrastructure for the sake of it.\n\nHope this helps.",
        "engagement": 266,
        "line_count": 19,
        "language": "English",
        "tags": [
            "System Design"
        ]
    }
]